# ğŸ‰ DPDEdit: Detail-Preserved Diffusion Models for Multimodal Fashion Image Editing
This is the official implementation of the paper "DPDEdit: Detail-Preserved Diffusion Models for Multimodal Fashion Image Editing".

## ğŸ“Œ Project Overview

![workflow](assets/workflow.png)

---

## ğŸ“· Results

![effect](assets/effect.png)

---


## âš™ï¸ Requirements

```
git clone https://github.com/XL-Wang1117/DPDEdit.git
cd DPDEdit

conda env create -f environment.yaml
conda activate dpdedit
```

## ğŸ“¦ Dataset
```
We evaluate and train DPDEdit on the DPDEdit-Extension dataset â€” an extension of VITON-HD with annotated texture-image pairs and garment descriptions.

You can download the dataset from:
ğŸ‘‰ [ğŸ”— Coming soon]

Expected folder structure:

DPDEdit-Extension/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ image/
â”‚   â”œâ”€â”€ image-densepose/
â”‚   â”œâ”€â”€ agnostic-mask/
â”‚   â”œâ”€â”€ cloth/
â”‚   â””â”€â”€ vitonhd_train_tagged.json
â”œâ”€â”€ test/
â”‚   â”œâ”€â”€ image/
â”‚   â”œâ”€â”€ image-densepose/
â”‚   â”œâ”€â”€ agnostic-mask/
â”‚   â”œâ”€â”€ cloth/
â”‚   â””â”€â”€ vitonhd_test_tagged.json
```
ğŸ”® Inference
```
Run inference on the test set:

accelerate launch Inference.py \
    --width 768 \
    --height 1024 \
    --num_inference_steps 30 \
    --output_dir "results" \
    --unpaired \
    --data_dir "PATH_TO_DPDEdit-Extension/test" \
    --seed 42 \
    --test_batch_size 2 \
    --guidance_scale 5.0
Replace PATH_TO_DPDEdit-Extension with your local dataset path.
```
## ğŸ‹ï¸ Training (optional)
```
To train the model from scratch or fine-tune:


accelerate launch train.py \
    --output_dir "checkpoints" \
    --train_batch_size 6 \
    --data_dir "PATH_TO_DPDEdit-Extension/train"
You can modify training configs via parser_args.py or configs/.
```


  title={DPDEdit: Detail-Preserved Diffusion Models for Multimodal Fashion Image Editing},
  author={Xiaolong Wang, Zhigi Cheng, Jue Wang, Huizi Xue, Xiaojiang Peng},
  booktitle={IEEE International Conference on Multimedia and Expo (ICME)},
  year={2025}
}
